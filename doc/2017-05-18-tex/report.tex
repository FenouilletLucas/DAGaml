\documentclass[a4paper,10pt]{article}
%\usepackage{fullpage}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[english]{babel}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{mathabx}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{hyperref}

\usepackage{cite}

\lstset{breaklines=true,
  mathescape=true,
	%language=caml,
	numbers=left,
  numberstyle=\tiny \bf, %\color{blue},
  %stepnumber=2,
  numbersep=10pt,
  %firstnumber=11,
  numberfirstline=true
	}

\title{A Generalized Reduction of Ordered Binary Decision Diagram}
\author{Joan Thibault}

\newcommand{\includeframe}[4]{\makebox[#2\linewidth]{\includegraphics[page=#1,width=#2\linewidth,trim=0cm 0cm 0cm 0cm,clip=true,#3]{#4}}}

\begin{document}

\maketitle
\begin{abstract}
Reduced Ordered Binary Decision Diagram ((RO)BDD)\cite{Bryant1986, Somenzi1999} are the state-of-the-art representation for Boolean functions.
They are used in various fields such as logic synthesis, artificial intelligence or combinatorics.
However, BDDs suffer from to main issues: (1) their representation is memory expansive and (2) their manipulation is memory intensive as it induces many random memory accesses.


Various variations of ROBDD exist such as Zero-suppressed Decision Diagram (ZDD)\cite{IntroZDD}, Multi-valued Decision Diagram (MDD)\cite{IntroMDD, IntroMddRolf} and variations of the reduction rules such as "output inverter"\cite{BryantVariantN}, "input negation"\cite{MinatoVariants}, "shifting variables"\cite{MinatoVariants}, "dual edges"\cite{RolfVariantDual} or "copy node"\cite{RolfVariantCopy}.


In this report, we introduce a new generalization of the standard reduction rules that we call the "extraction of useless variables" or "extract U" for short.
Basically, it detects useless variables (i.e. variables which have no influence on the result of a given function) and extracts them from the local variable order.
This generalization allows to add/remove useless variables in linear time (in the number of variables), reduces the number of nodes and tends to reduce the overall memory cost.
However, several drawbacks arise: no in-place sifting (permutation of adjacent variables), bigger nodes of variable size and it slightly complexifies manipulations.


We implemented both the "extract U" and the "output negation" variants in an OCaml program and tested it against several benchmarks\cite{BenchSatlib, BenchLgsynth91, BenchIscas99}.
We observe an average of 25\% less nodes and 32\% less memory when representing circuits, and 3\% less nodes and memory when representing solutions from generated CNF formulas.
\end{abstract}


\newpage
\tableofcontents
\newpage
\newcommand{\shannon}[3]{#1 \longrightarrow_S #2, #3}
\newcommand{\N}{\mathbb{N}}%{\{0, 1\}}
\newcommand{\B}{\mathbb{B}}%{\{0, 1\}}
\newcommand{\F}{\mathbb{F}}%{\{0, 1\}}

\section{Introduction}

Nowadays, it exists many critical systems which rely on digital circuits: in transportation (e.g. cars, train, plains) , communication (e.g. satellites), computation (e.g. data centers, super-computers), exploration (e.g. space rocket, rovers).
One way of minimizing risks in digital parts of these systems is to provide a formal proof that they respect their specification.
On the other hand, we want to minimize costs and energy consumption while maximizing performances of these digital circuits.
In order to efficiently optimize digital circuits we rely on complex programs.
However, these programs are rarely proven themselves, thus, circuits optimized using them might not be equivalent to the initial design, therefore, might not respect the specification.
The obvious solution would be to prove optimizing programs, however two majors issues arise : these programs are complex  (thus, proving them would be expansive) and might be proprietary (thus, one cannot check that the proof is correct).
A simpler alternative is to design a program which check that two digital circuits are equivalent.
With this alternative, the only piece of software which needs to be proven is the "equivalence checker".


In order to prove that two digital circuits are equivalent, their is two main algorithmic solutions:
Firstly, the DPLL (Davis–Putnam–Logemann–Loveland) algorithm.
This backtracking procedure is usually implemented with various heuristics such as \textit{unit propagation}, early conflict detection or \textit{conflict driven clause learning}.
Secondly, the compilation of both circuits into Reduced Ordered Binary Decision Diagrams (ROBDDs).
A ROBDD is a canonical structure which represent a function, thus, once compiled, the identity test can be performed in constant time.
However, the compilation might take an exponential time in the number of variable.
In this report we will focus on ROBDDs.


ROBDD have various other applications such as: Bounded Model Checking, Planning, Software Verification, Automatic Test Pattern Generation, Combinational Equivalence Checking or Combinatorial Interaction Testing.

%\begin{figure}
%\[f_n(x_0, x_1, \dots, x_{2n}, x_{2n+1}) = (x_0 \land x_1) \lor \dots \lor (x_{2n} \land x_{2n+1})\]
%\[g_n(x_0, x_1, \dots, x_{2n}, x_{2n+1}) = f_n(x_0, x_{n+1}, \dots, x_n, x_{2n+1})\]
%\caption{The ROBDD representing the function $f_n$ has a linear size in $n$, the one representing $g_n$ has a an exponential size in $n$.}
%\label{OrderExplosion}
%\end{figure}


However, BDDs are memory expansive as their size tends to grow exponentially with the number of variables.
Various variants have been invented in order to capture some semantic properties of the function and reduce the memory consumption.
For example, Zero suppressed binary Decision Diagram (ZDD) are better suited for representing sparse functions.
In this report we will use the "output inverter" variant\cite{BryantVariantN}, which extends the reduction rules in order to guarantee canonicity under negation.
Thus, in addition to reduce the size of the structure, allows to negate a function in constant time (reducing the set of useful binary operators to XOR and AND).
Other extensions of the reduction rules exist such as: "input negation"\cite{MinatoVariants} (each edge can complement the first locally first input), "shifting variables"\cite{MinatoVariants} (each edge store the number of useless variables before the next significant variables) or "dual edge"\cite{RolfVariantDual} (we define the dual of a function $f$ by $\bar{f} = X \longrightarrow \bar{f}(\bar{X})$, therefore the reduction works similarly to the "output inversion").


In addition to use the "output inverter" variant, we introduce a new variant which allows to extract useless variables (a.k.a non-support variables).
A useless variables, is a variable which does not change the results such as $x_1$ in $f(x_0, x_1, x_2) = x_0 \land x_2$ or $x_0$ in $g(x_0) = x_0 \land \lnot x_0$.
We call this new variant "extraction useless variables" or "U-extract" for short.


This report will be organized as follows. In Section~1, we formally introduce Boolean functions and useless variable. In Section~2, we introduce ROBDDs. In Section~3, we introduce the "U-extract" variant and prove that it maintains the canonicity. In Section~4, we expose an estimations of improvements on three different benchmarks \cite{BenchSatlib, BenchLgsynth91, BenchIscas99} using our implementation in OCaml.

\section{Notations}

Reduced Ordered Binary Decision Diagrams represent Boolean functions.
In this section we introduce notations necessary to their manipulation.


We denote the set of Booleans $\B = \{0, 1\}$.
The set of Boolean vector of size $n\in\N$ is denoted $\B^n$.
The set of Boolean functions of arity $n\in\N$ is denoted $\F_n = \B^n \longrightarrow \B$.


We denote conjunction by $\land$, disjunction by $\lor$, negation by $\lnot$.
We denote the Shannon operator by $\shannon{}{}{}$ defined by $\forall x, y, z\in\B, \shannon{x}{y}{z} = (\lnot x \land y) \lor (x \land z)$).

\paragraph{Restriction\\}

Let $f\in\F_{n+1}$ be a Boolean function of arity $n+1$, $i~(0\leq i < n+1)$ be an integer and $b\in\B$ be a Boolean.
We denote $f[i\leftarrow b]$ the Boolean function of arity $n$ defined by $f[i\leftarrow b](x_1, \dots, x_n) = f(x_0, \dots, x_{i-1}, b, x_i, \dots, x_n)$.
$f[i\leftarrow 0]$ (respectively $f[i\leftarrow 1]$) is called the $i$-th negative (respectively positive) restriction.

For each Boolean function $f$ of arity $n+1$, we denote $f[i\leftarrow b] = (x_1, ..., x_n) \rightarrow f(x_1, .., x_{i-1}, b, x_i, ..., x_n)$ the function of arity n called the i-th positive restriction of $f$ if $b=1$, the negative one otherwise.


Let $f$ be a function of arity $n+1$, we denote $f_0 = f[0\leftarrow 0]$ (respectively $f_1 = f[0\leftarrow 1]$) the function of arity $n$.

\paragraph{Construction\\}

Let $f, g \in F_n$ be Boolean functions of arity $n$ and $i~(0\leq i < n+1)$ be an integer.
We denote $f\star_ig$ the Boolean function of arity $n+1$ defined by $(f\star_ig)(x_0, \dots, x_{i-1}, y, x_i, \dots, x_n) = \shannon{y}{f(x_0, \dots, x_n)}{g(x_0, \dots, x_n)}$.

We denote $\star = \star_0$

Nb: $(f\star_i g)[i\leftarrow 0] = f$ and $ (f\star_i g)[i\leftarrow 1] = g$ ($(f\star g)_0 = f$ and $ (f\star g)_1 = g)$.

\paragraph{Expansion Theorem\\}
Let $f$ be a function of arity $n$, then $\forall i, 0\leq i < n \Rightarrow f = f[i\leftarrow 0]\star_i f[i\leftarrow 1]$ (in particular $f = f_0 \star f_1$)

\paragraph{Restriction Distributivity\\}

\begin{itemize}
\item $(\lnot f)[i\leftarrow b] = \lnot f[i\leftarrow b]$
\item $(f\land g)[i\leftarrow b] = f[i\leftarrow b] \land g[i\leftarrow b]$
\item $(f\lor g)[i\leftarrow b] = f[i\leftarrow b] \lor g[i\leftarrow b]$
\end{itemize}

\paragraph{Useless Variables\\}

We define, the support set of a function $f$, the set of variable index $i$ such that $f[i\leftarrow 0] \neq f[i\leftarrow 1]$.
A variable that does not belong to the support, is said to be a non-support variable or useless variable.
Thus, the $i$-th variable of $f$ is useless iff $f[i\leftarrow 0] = f[i\leftarrow 1]$.

\section{Reduced Ordered Binary Decision Diagram (ROBDD) and Canonicity}

\paragraph{Definition of Binary Decision Diagram (BDD)\\}

A Reduced Ordered Binary Decision Diagram is a directed acyclic graph $(V\cup T, \Psi \cup E)$ representing a vector of Boolean functions $F=(f_1, ..., f_k)$ other an infinite set of variables.
Nodes are partitioned into two sets : the set of internal nodes $V$ and the set of terminal nodes $T$.
Every internal node $v\in V$ has one field $var$, which represent the index of a variable and two outgoing edges respectively denoted $if0$ and $if1$.
When using the "output inverter" variant, there is only one terminal called 0, which represent the functions which always return 0.
Arcs are partitioned into two sets : the set of root arcs $\Psi$ and the set of internal arcs $E$.
There is exactly $k$ root arcs, a root arc is denoted $\Psi_i$ with $0\leq i < k$, informally, $\Psi_i$ is the root of the ROBDD representing $f_i$.
Every arc has an inversion field $neg \in \B$ and a destination node denoted $node$.

%add beautiful draw (with \psi arcs)$

We denote $\phi(node)$ the semantic of the node $node$ and $\psi(arc)$ the semantic of the arc $arc$ as follow:\begin{itemize}
\item $\forall i, f_i = \psi(\Psi_i)$
\item $\forall arc \in \Psi \cup E, \psi(arc) = arc.neg \oplus \phi(arc.node)$
\item $\phi(0 \in T) = 0$
\item $\forall node \in V, \phi(node) = \shannon{node.var}{\psi(node.if0)}{\psi(node.if1)}$
\end{itemize}

\paragraph{Definition of Reduced Ordered BDD (ROBDD)\\}
A BDD is said \texttt{ordered} if (1) $\forall v\in V$, $v.then.node \in V \Rightarrow v.var > v.if1.node.var$ and $v.else.node \in V \Rightarrow v.var > v.if0.node.var$.
%define an order over variables

A BDD is said \texttt{reduced} if (2) $\forall v\in V, v.if0 \neq v.if1$ and (3) every node has an in-degree strictly positive.

\paragraph{Theorem : ROBDD are canonical\\}

Let consider a ROBDD $G$ representing $F=(f_1, ..., f_n)$ over a set of n variables $x_n < x_{n-1} < ... < x_1$. Then, for every nodes $v_1, v_2 \in G$, $\phi(v_1) = \phi(v_2) \Leftrightarrow v_1 = v_2$.

A proof a proof of this theorem, is available in the review of Somenzi et al.\cite{Somenzi1999}.

\subsection{Basic Manipulation of Binary Decision Diagram (BDD)}



\subsubsection{Effective construction}
In practice one does not build the decision tree and then reduces it.
Rather, BDDs are created starting from the BDDs for the constants and the variables by application of the usual Boolean connectives and are kept reduced at all times.
At the same time several functions are represented by one multi-rooted diagram.
Indeed, each node of a BDD has a function associated with it.
If we have several functions, they will have subfunctions in common.
For instance, if we have $f(x_0, x_1, x_2, x_3) = \shannon{x_1}{x_2}{x_3}$ and $f(x_0, x_1, x_2, x_3) = \shannon{\lnot x_1}{x_2}{x_3}$. As a special case two equivalent functions are represented by the same BDD (not just two identical BDDs).
This approach makes equivalence check a constant-time operation.
Its implementation is usually based on a dictionary of all BDDs nodes in existence in an application.
This dictionary is called the \textit{unique table}.
Operations that build BDDs start from the bottom (the constant nodes) and proceed up to the function nodes.
Whenever an operation needs to add to a BDD that it is building, it knows already the two nodes (say $f_1$ and $f_0$ represented by some identifier (usually implemented as pointers)) that going to be the new node's children and the decision variable $v$ so it just has to check if the node $(v, f_1, f_0)$ already exist and if so return its identifier and if not generate a new identifier and return it.
Doing so, the equivalence check is reduced to pointer comparison.

\paragraph{Operator \texttt{CONS}\\}


Pseudo-code for dynamic construction of unique nodes.

\begin{lstlisting}
let cons var if0(*else arc*) if1(*then arc*) =
  if if0 = if1
  then if0
  else
  (
    if0' = {neg = 0; node = if0.node}
    if1' = {neg = if0.neg $\oplus$ if1.neg; node = if1.node}
    mynode = {var; then = if0'; else = if1'}
    myid = if mynode in $\mathit{unique~table}$
      then "mynode's identifier"
      else
      (
        store mynode in $\mathtt{unique table}$;
        "mynode's identifier"
      )
    {neg = if0.neg; node = myid}
  )
\end{lstlisting}


\subsubsection{Operators}

\newcommand{\op}{~op~}

The usual way of generating new BDDs is to combine existing BDDs using operators such as conjunction $AND$, symmetric difference $XOR$ and negation $NOT$ (this one is performed in constant time by complementing the $neg$ field of the root arc representing the function).
One starts with simple BDDs for the function $f_i = x_i$, for all the variables in the functions of interest.
Starting with BDDs representing $f$ and $g$, we inductively compute $f\op g$, by applying the expansion theorem and the distributivity rules.
\[f\op g = (f\op g)_0 \star (f\op g)_1 = (f_0\op g_0)\star(f_1 \op g_1)\]

Bryant \cite{Bryant1986} proved that, if the inductive process is memoized (i.e. we store intermediary results) the asymptotic complexity of the inductive algorithm if $O(N_f \times N_g)$ (with for all Boolean function $f$, $N_f$ is the number of node in the ROBDD representing it).

\paragraph{Example : operator AND\\}
The terminal cases for this operator are:\begin{itemize}
\item $AND(f, 0) = AND(0, f) = 0$
\item $AND(f, 1) = AND(1, f) = f$
\item $AND(f, f) = f$
\item $AND(f, \lnot f) = 0$
\end{itemize}

These conditions can be computed in constant time, then we use the \texttt{memoization table} to check if the result has already been computed, and if not we apply the \texttt{expansion theorem} and solve the problem inductively.

\begin{lstlisting}
AND(f, g){
  if f > g{
    return AND(g, f)
  }else if "terminal case" {
    return terminal(f, g)
  }else if "memoization table has entry (f, g)" {
    return memoization((f, g))
  }else{
    let x be the top variable of {f, g}
    f1, f0 = cofactor(x, f)
    //if f does not depends on x, then f1=f0=f
    g1, g0 = cofactor(x, g)
    //idem for g
    //but either f or g has to depend on x
    r1 = AND(f1, g1)
    r0 = AND(f0, g0)
    r = CONS(x, r1, r0)
    insert {key = (f, g); value = r} in memoization
    return r
    }
}
\end{lstlisting}

n.b.: computing $NOT(f)$ can be done in constant time because of the "output inverter" variant.

\paragraph{Operator XOR}
Very similar to AND with some differences in terminal case:\begin{itemize}
\item $XOR(0, f) = XOR(f, 0) = f$
\item $XOR(1, f) = XOR(f, 1) = NOT(f)$
\item $XOR(f, f) = 0$
\end{itemize}

\section{Introduction of Generalized Reduction of Ordered Binary Decision Diagram (GroBdd)}

\subsection{Motivation}

In one hand, a ROBDD can be understood as an automaton recognizing a language compose of binary words.
On the other hand, a ROBDD can be understood as a logic circuit with limited conciseness.
By limited conciseness, we mean that their is functions which have a polynomial representation using an And-Inverter-Graph (AIG, i.e. a logic circuit composed of \texttt{AND} and \texttt{NOT} gates), but only exponential representation when using ROBDD.
For example, the integer multiplication has a quadratic AIG representing it, however, it is proven \cite{Bryant1986} that a ROBDD representing it has at least an exponential number of nodes (thus, of gates).

The "output inverter" variant, by adding expressiveness to edges, improves the conciseness.
We want to go further in this approach by allowing more complex transformation on edges while maintaining canonicity.
Such transformation can disturb variables' order of evaluation, allowing to represent "simple" decision processes in more concise way.
Furthermore, some transformation (within the range of transformation allowed) could have their complexity drastically reduced.
For example, using the "output inverter" variant, the complementation's time and space complexity which goes from linear in the number of node to constant.

Three set of transformation that would be of great interest are the "useless variable extraction" (or "U extract" for short), the "inputs inverters" and "1-predictions extraction" (or "X extract" for short).


The "U extract" variant allows to introduce useless variables, thus ensure that the function represented by any node, does not have useless variable.
Therefore, it sets an upper bound on the number of node of arity $n$ to $2^{2^n}$ (this upper bound is not reached).
Furthermore, it allows to "copy" functions at (almost) no cost as the expression $f(x_1, x_3, x_5, x_7)$ and $f(x_0, x_1, x_3, x_5)$ are represented using the same structure.


The "inputs inverters" variant would allow to complement inputs on any edge.
The reduction rules would ensure that there is at most $2^{2^n-n}$ nodes of arity $n$ (this upper bound is not reached).
However, introducing this variant breaks the canonicity, therefore, while working on a generalization of it, we do not discuss it further in this report.

We define a 1-prediction as follow:
Let $f$ be a Boolean function of arity $n$, $i$ be an integer ($0\leq i < n$), $x$ and $y$ be Booleans, we say that the function $f$ admit a 1-prediction $(i, x, y)$ iff $f[i\leftarrow x] = y$.
The "X extract" variant, by allowing to extract 1-predictions, represents a generalization of ZBDD (Zero Suppressed Binary Decision Diagram), thus, allows to efficiently represent sparse function.
This variant is compatible with the "output inverter" variant, however it over-complexifies the reduction rules, therefore, while reformulating reduction rules in a simpler way, we do not discuss it further in this report.


\subsection{Definition of GroBdd}

A GroBdd is very similar to an actual ROBDD, the two main differences being that (1) it represents a vector of Boolean functions with a finite number of variables possibly of different arity and (2) on every edge their is a transformation.

A Reduced Ordered Binary Decision Diagram is a directed acyclic graph $(V\cup T, \Psi \cup E)$ representing a vector of Boolean functions $F=(f_1, ..., f_k)$.
Nodes are partitioned into two sets : the set of internal nodes $V$ and the set of terminal nodes $T$.
Every internal node $v\in V$ has two outgoing edges respectively denoted $if0$ and $if1$.
Arcs are partitioned into two sets : the set of root arcs $\Psi$ and the set of internal arcs $E$.
There is exactly $k$ root arcs, a root arc is denoted $\Psi_i$ with $0\leq i < k$, informally, $\Psi_i$ is the root of the GroBdd representing $f_i$.
Every arc has a transformation descriptor field $\delta$ and a destination node denoted $node$.

%add beautiful draw (with \psi arcs)$

We denote $\phi(node)$ the semantic of the node $node$ and $\psi(arc)$ the semantic of the arc $arc$ as follow:\begin{itemize}
\item $\forall i, f_i = \psi(\Psi_i)$
\item $\forall arc \in \Psi \cup E, \psi(arc) = \rho(arc.\delta)(\phi(arc.node))$
\item $\phi(0 \in T) = 0$
\item $\forall node \in V, \phi(node) = \psi(node.if0) \star \psi(node.if1)$
\end{itemize}


\section{Introduction of Reduced Decision Tree (RDT) and Canonical Shared Forest (CSF)}
\subsection{Motivation}
Adding attribute on arcs allowed us to (slightly) reduce computation time and space.
In order to go further in this direction we have to introduce more formalism.
In order to simplify reasoning we will split the current structure in two:\begin{itemize}
\item the Canonical Shared Forest (CSF), which job is to ensure structural canonicity (i.e. attribute an identifier to every sub-trees and ensure that any identical sub-trees have the same identifier).
\item the Reduced Decision Tree (RDT), which job is to ensure semantic canonicity (i.e. ensure that the semantic of operation is correct and that a function has exactly one representation)
\end{itemize}
In the following sections we describe the role of both these structures and how they work together.

\subsection{From the \texttt{unique table} to the Canonical Shared Forest (CSF)}
The Canonical Shared Forest (CSF) encapsulates the \texttt{unique table} and the \texttt{CONS} operator in one structure.
Its job is to hide from the RDT the whole memory management.
In its basic implementation interactions are reduced to two methods:\begin{itemize}
\item \texttt{push} (previously called \texttt{CONS}) that takes a node and returns a unique identifier.
\item \texttt{pull} that take an identifier and return the associated node.
\end{itemize}
A more complex implementation could use some commands in order to specify when a node is no longer required in order to save memory.
In order to save time and extend memory we can think about using distributed memory units, efficiently using of the memory hierarchy or allow concurrent/parallel accesses.
However such implementation details fall out of the scope of this lecture.

NB: Implementing the CSF can lead to theoretical issues (especially when dealing with distributed, concurrent or parallel versions) but they are not specific to our problem.

\subsection{From Shannon's Decision Tree to Reduced Decision Tree (RDT)}
The Reduced Decision Tree (RDT) is the theoretical (and computational) core of the compression system.
The basic is to represent sets of somehow related functions by a common graph instead of distinct ones.

\subsubsection{Change in representation}

\paragraph*{Definition : Syntax\\}

We define a Compressed BDD by a graph $G = (V\cup T, \Psi\cup E)$ where:\begin{itemize}
  \item there are two sets of vertices:\begin{itemize}
    \item $V$, the set of internal nodes. Internal nodes have an out-degree of two : its sons are called $then$ and $else$.
    \item $T$, the set of terminal nodes. Terminal nodes have an out-degree of zero.
  \end{itemize}
  \item there are two set of arcs:\begin{itemize}
      \item $E$ the set of internal arcs which have a source node called $source$.
    \item $\Psi$ the set of root arcs which has no source node.
  \end{itemize} arcs in both sets have a destination called $node$ and carry some data called $coreduce$.
\end{itemize}
%put a beautiful draw with \psi arcs (no source)$

\paragraph*{Definition : Semantic\\}

We denote $\phi(node)$ the semantic of the node $node$ and $\psi(arc)$ the semantic of the arc $arc$ as follow:\begin{itemize}
\item $\forall node\in T$, $\phi(node) = f_{node}$ (usually $\phi(0) = () \rightarrow 0 \in \B^0 \rightarrow \B$)
\item $\phi(node) = \psi(node.then)\star_i\psi(node.else)$
\item $\psi(arc) = \rho(arc.coreduce)(\phi(arc.node))$ for some function $\rho$ such that $\rho(arc.coreduce) : (\B^k \rightarrow \B) \rightarrow (\B^n \rightarrow \B)$ with $k\leq n$. $k$ (respectively $n$) is called the in-arity (respective out-arity) of $arc.coreduce$.
\end{itemize}

\paragraph{Reduction Rules}

A CBDD is said syntactically reduced if it satisfies (1) some local property $P$ on nodes (and their two outgoing arcs) (2) there is no identical (up to isomorphism) sub-graph.


A CBDD is said semantically reduced if $\forall v_1, v_2 \in V\cup T, \phi(v_1)=\phi(v_2) \Rightarrow v_1 = v_2$.

\paragraph{Canonicity}


We say that the local property $P$ ensures pseudo-canonicity if any syntactically reduced CBDD is semantically reduced.


We say that the local property $P$ ensure full-canonicity (or the canonicity) if for all functions $f$, every representations of $f$ are equal up to graph isomorphism.


\subsubsection{Reduction in constructor}

\texttt{RDT.CONS}'s goal is to ensure that $P$ is verified when building a new internal node.
We may notice that it has three distinct behaviors:\begin{enumerate}
  \item the result can be build only with constant nodes and the two available nodes.
  \item the result needs a node that has already been build.
  \item the result needs a new node.
\end{enumerate}
In both cases 2 and 3, an access to CSF is needed.

\begin{lstlisting}
RDT.CONS(arc1, arc0){
  match RDT-consensus(then, else, cmp) with
    | Solved result $\longrightarrow$ result
    | Partial (coreduce, reduce) $\longrightarrow$
      arc(coreduce, CSF.PUSH(reduce))
}
\end{lstlisting}

\texttt{RDT.SPLIT} performs the reverse operation of \texttt{RDT.CONS}. We may notice that it has two distinct behaviors:\begin{itemize}
  \item the result can be computed without accessing to CSF
  \item the result is computed accessing CSF.
\end{itemize}

\begin{lstlisting}
RDT.SPLIT(arc)
  if(terminal case){
    return result
  }else{
    reduce = CSF.PULL(arc.reduce-ident)
    arc1 = compose(arc.coreduce, reduce.then)
    arc0 = compose(arc.coreduce, reduce.else)
    return (arc1, arc0)
  }
}
\end{lstlisting}
\subsubsection{Reduction in operators}

In order to compute $AND$, there are cases:\begin{itemize}
  \item its a terminal case, so we can solve it immediately
  \item its a memoized case, so we can return the result from memory
  \item We can split the problem into two sub-problems (which can be solved inductively).
\end{itemize}
We slightly "rotate" these choices in order to abstract the memoization process

\begin{lstlisting}
AND-SOLVE(reduced-problem){
  arcX, arcY = and-unpack(reduced-problem)
  arcX1, arcX0 = RDT.SPLIT(arcX)
  arcY1, arcY0 = RDT.SPLIT(arcY)
  arc1 = match and-solver(arcX1, arcY1) with
    | Solved result $\longrightarrow$ result
    | Partial (coreduce, subproblem) $\longrightarrow$
    	compose(coreduce, AND-SOLVE(subproblem))
  arc0 = match and-solver(arcX0, arcY0) with
    | Solved result $\longrightarrow$ result
    | Partial (coreduce, subproblem) $\longrightarrow$
    	compose(coreduce, AND-SOLVE(subproblem))
  return RDT.CONS(arc1, arc0)
}
\end{lstlisting}
\texttt{AND-SOLVE} is a recursive memoized algorithm (i.e. when AND-SOLVE(problem) returns value, we store an entry into a dictionary and when calling AND-SOLVE, we first look into the dictionary to know if the result is already available and if so return it).
\begin{lstlisting}
RDT.AND(arcX, arcY){
  match and-solver(arcX, arcY) with
    | Solved result $\longrightarrow$ result
    | Partial (coreduce, subproblem) $\longrightarrow$
    	compose(coreduce, AND-SOLVE(subproblem))
}
\end{lstlisting}



\section{Pattern extraction : useless variables}
\begin{figure}
\centering
%\caption{BDD with Extraction of useless variables}
\includeframe{3}{1}{}{draws.pdf}
\caption{Same BDD as in Figure~\ref{draws1}, when detecting and removing useless variables. Each red rectangle contain the support set of each arc's function.}
\label{draws3}
\end{figure}


\begin{figure}
\centering
%\caption{BDD with Complemented Arcs reduction \& Extraction of useless variables}
\includeframe{4}{1}{}{draws.pdf}
\caption{Same BDD as in Figure~\ref{draws2} and~\ref{draws3}, with both additional reduction rules : complemented arcs and extraction of useless variables.}
\label{draws4}
\end{figure}

\subsection{Motivation}
We may notice that the respective representations of $f_1 = f(x_{i_1}, ..., x_{i_n})$ ($i_1 < ... < i_n$) and $f_2 = f(x_{j_1}, ..., x_{j_n})$ ($j_1 < ... < j_n$) are very similar.
The only difference being the values of the $var$ field of each nodes within the representation of $f$.
This suggest the possibility of actually using the same sub-graph to represent $f$, $f_1$ and $f_2$.


Suppose we have a BDD representing $f$, then in order to represent $f_1$ we just have to remember which variable are in the support of $f$ (the relative order of variables in the support does not change).
In this attribute, we will store the support set in the $coreduce$ field of arcs.

Removing useless variables slightly complexify (e.g. we have to add an other reduction rule) the manipulation of BDDs, but has three advantages:\begin{itemize}
\item reduce memory usage (in some cases there is an exponential gain, but on most cases its less significant).
\item copying a function (if relative order of variables is unchanged) can be performed in constant time.
\item checking $g = f$ modulo their useless variables -when having $g$ and $f$- can be performed in constant time (not very useful).
\end{itemize}

\subsection{Proof of Canonicity}

We define the following reductions rules:\begin{itemize}
  \item $\forall v\in V, v.then.coreduce \cup v.else.coreduce = \{0, ..., n\}$
  \item $\forall v\in V\cup T, \phi(v)$ has no useless variable
\end{itemize}

For every function $f$, we denote $\tilde{f}$ the restriction of $f$ to its support (maintaining the relative order of the remaining variables).

\paragraph{Existence}
Let $f\in\B^n\rightarrow\B$ be a Boolean Function. We recursively build a valid representation $R(f)$ of $f$ which verify the reduction rules:\begin{itemize}
  \item $R((x_1, ..., x_n) \rightarrow b) = arc(coreduce = \emptyset, node = 1)$, where $b\in\B$
  \item $R(f) = $\begin{itemize}
    \item We denote $S=\{i_1 < ... < i_k\}$ the support set of $f$.
    \item $R(f) = arc(coreduce = S, node = node(then = R(\tilde{f}[0\leftarrow 1]), else = R(\tilde{f}[0\leftarrow 0])))$
  \end{itemize}
\end{itemize}

\paragraph{Uniqueness}
We prove canonicity by induction on $n$ the arity of the represented function.
\subparagraph{Initialization}
Let $R$ be a representative of a function of arity 0. We consider the root arc $a$, so $a.coreduce = \emptyset$ (the only set of cardinality lower than zero). However the \texttt{in-arity} of $\emptyset$ is 0, so $a.node$ represent a constant function. Hence either $R = arc(coreduce = \emptyset, node = 0)$ representative of the constant function 0 or $R = arc(coreduce = \emptyset, node = 1)$ representative of the constant function 1.
\subparagraph{Induction}
Assuming that the hypothesis holds for every $0\leq k\leq n$.
Let $R$ be a representative of $f$, we denote $a$ its root arc.
\begin{itemize}
  \item We assume that $a.coreduce \neq \{0, ..., n\}$. Then the arity of $g = \phi(a.node)$ is lower strictly lower than $n$. However $g$ has a unique representation (induction hypothesis) and $g$ has no useless variable (because of the reduction rules on applied on node $a.node$). So the only useless variable in $f$ are the one that does not appear in $a.coreduce$. Therefore $R$ is unique.
  \item We assume that $a.coreduce = \{0, ..., n\}$. We denote $a_1$ (respectively $a_0$) the root arc of the representation (unique by induction hypothesis) of $f_1 = \psi(a.node.then)$ (respectively $f_0 = \psi(a.node.else)$).\begin{itemize}
    \item We assume that $f_1 = f_0$ then $a_1 = a_0$ then either induce that $0\not\in a.coreduce$ (which induce a contradiction with $S=\{0, ..., n\}$) or R does not satisfies the reduction rules.
    \item Therefore $f_1\neq f_0$.\begin{itemize}
      \item We assume that $a_1.coreduce~\cup~a_0.coreduce \neq \{0, ..., n-1\}$. Then it exist $0\leq i\leq n-1$ such that $x_i$ is useless for both $f_1$ and $f_0$. Hence $i+1$ is useless for $f$. So R does not satisfies the reduction rules.
      \item Therefore $a_1.coreduce~\cup~a_0.coreduce = \{0, ..., n-1\}$. Then $a = arc ( coreduce = \{0, ..., n\},~node = node(then = a_1,~else = a_0)$. Therefore $R$ is unique.
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection{Reduction in construction}
Moreover, as we already define the main algorithm for construction, we just have to write the specific sub-routines:
\begin{lstlisting}
RDT-consensus($then$, $else$, cmp){
  if (cmp = 0) & $then.coreduce$ = $else.coreduce$ then{
    we denote $\{i_1 < ... < i_k\} = then.coreduce$
    Solved arc(then = $\{i_1+1 < ... < i_k+1\}$, node = then.node)
  }else{
    we denote $U = \{i_1 < ... < i_k\} = then.coreduce \cup else.coreduce$
    we denote $A = \{a_1 < ... < a_l\}$ the indexes of $then.coreduce$ in $U$.
    we denote $B = \{b_1 < ... < b_m\}$ the indexes of $else.coreduce$ in $U$.
    Partial (coreduce = $\{0, i_1+1, ..., i_k+1\}$, reduce = node(then = arc(coreduce = $A$, node = $then.node$), else = arc(coreduce = $B$, node = $else.node$) ) )
  }
}  
\end{lstlisting}
\begin{lstlisting}
compose$(\{i_1 < ... < i_n\}$, arc(coreduce = $\{j_1 < ... < j_k\}$, node = node){
  return arc(coreduce = $\{i_{j_1} < ... < i_{j_k}\}$, node = node)
}
\end{lstlisting}

\subsection{Reduction in operators}
Moreover, as we already define the main algorithm for computing AND, we just have to write the specific sub-routines:
\begin{lstlisting}
and-unpack(reduced-problem){
  return (reduced-problem.arcX, reduced-problem.arcY)
}
\end{lstlisting}
\begin{lstlisting}
and-solver(arcX, arcY){
  if (terminal cases){
    return result //removing useless variables does not add terminal cases
  }else{
    we denote $U = \{i_1 < ... < i_k\} = then.coreduce \cup else.coreduce$
    we denote $A = \{a_1 < ... < a_l\}$ the indexes of $then.coreduce$ in $U$.
    we denote $B = \{b_1 < ... < b_m\}$ the indexes of $else.coreduce$ in $U$.
    if arcX $\leq$ arcY
    then{
      Partial (coreduce = U, subproblem = (arcX = arc(coreduce = A, node = arcX.node), arcY = arc(coreduce = B, node = arcY.node)))
    }else{
      Partial (coreduce = U, subproblem = (arcY = arc(coreduce = A, node = arcX.node), arcX = arc(coreduce = B, node = arcY.node)))
    }
  }
}
\end{lstlisting}


\section{Conclusion}

ROBDD allows to efficiently manipulate functions appearing in various fields of computer science such as: Bounded Model Checking, Planning, Software Verification, Automatic Test Pattern Generation, Combinational Equivalence Checking or Combinatorial Interaction Testing.

However, ROBDD manipulation is memory intensive and various variants exist (such as "output inverter") in order to reduce the memory cost.

In this report, we introduce a new variant called "useless variable extraction" (a.k.a. "U-extract" for short).
This variant, by capturing useless variable, allows to reduce the number of nodes.


Using our implementation in OCaml against several benchmarks, we obtain an average reduction of 25\% when representing circuits, and 3\% when representing generated CNF formula.
Furthermore, we estimated the memory cost of this variant, in average, we observe a 30\% reduction when representing circuits and 3\% when representing CNF formula.


\newpage
\bibliography{biblio}{}
\bibliographystyle{plain}
\end{document}
